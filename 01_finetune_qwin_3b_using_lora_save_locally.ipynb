{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpQUzplJ9FQb"
   },
   "source": [
    "# Fine tune a Base Model (5 steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mhaAp2o4vJs"
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# LoRA fine-tuning requires PEFT, Transformers, Datasets, and Accelerate\n",
    "# bitsandbytes is for 4-bit quantization (QLoRA)\n",
    "!pip install -q -U transformers datasets peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1756377039607,
     "user": {
      "displayName": "Basharat Hussain",
      "userId": "14124140969393964043"
     },
     "user_tz": -300
    },
    "id": "5ANIzDQw45mJ",
    "outputId": "c97a86ad-e750-46b5-e257-90f51cc5e31b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 28 10:30:39 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   41C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkbMf8om5fgC"
   },
   "outputs": [],
   "source": [
    "\n",
    "# A small 3B model for this example.\n",
    "# We load it in 4-bit to save memory (QLoRA)\n",
    "MODEL_NAME = \"Qwen/Qwen2-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzJTK31r49jp"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# Hugging Face login\n",
    "hf_token = \"hf_iGWjRtjobCydZbVOTLOXXXXXXXX\"  # Replace with your HF token\n",
    "login(hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "executionInfo": {
     "elapsed": 3387,
     "status": "ok",
     "timestamp": 1756138044032,
     "user": {
      "displayName": "Basharat hussain",
      "userId": "01707569703308499646"
     },
     "user_tz": -300
    },
    "id": "ZkqhhSCm5YXJ",
    "outputId": "80e6d97d-5e26-490f-f700-83652bac6a79"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▃▄▅▅▆▆▇██</td></tr><tr><td>train/grad_norm</td><td>█▂▂▁▂▃▁▂▂▂▁▂</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▅▄▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▁▂▁▁▂▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>2019377283072000.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>125</td></tr><tr><td>train/grad_norm</td><td>0.2422</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.5252</td></tr><tr><td>train_loss</td><td>0.85128</td></tr><tr><td>train_runtime</td><td>229.815</td></tr><tr><td>train_samples_per_second</td><td>2.176</td></tr><tr><td>train_steps_per_second</td><td>0.544</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">qlora-Qwen2-1.5B-Instruct</strong> at: <a href='https://wandb.ai/basharat-hussain-78/my-finetuning/runs/fpbn09fs' target=\"_blank\">https://wandb.ai/basharat-hussain-78/my-finetuning/runs/fpbn09fs</a><br> View project at: <a href='https://wandb.ai/basharat-hussain-78/my-finetuning' target=\"_blank\">https://wandb.ai/basharat-hussain-78/my-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250825_155330-fpbn09fs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250825_160722-xwxp87aa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/basharat-hussain-78/my-finetuning/runs/xwxp87aa' target=\"_blank\">qlora-Qwen2-1.5B-Instruct</a></strong> to <a href='https://wandb.ai/basharat-hussain-78/my-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/basharat-hussain-78/my-finetuning' target=\"_blank\">https://wandb.ai/basharat-hussain-78/my-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/basharat-hussain-78/my-finetuning/runs/xwxp87aa' target=\"_blank\">https://wandb.ai/basharat-hussain-78/my-finetuning/runs/xwxp87aa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/basharat-hussain-78/my-finetuning/runs/xwxp87aa?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7e5260420200>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w and b settins\n",
    "import os\n",
    "# Configure WANDB\n",
    "os.environ[\"WANDB_API_KEY\"] = \"6f8cc13db6ea6485b46be0edda8225XXXXXXX\"\n",
    "import wandb\n",
    "wandb.init(project=\"my-finetuning\", name=f\"qlora-{MODEL_NAME.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAd0QiR65FZa"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import the required libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18670,
     "status": "ok",
     "timestamp": 1756138062714,
     "user": {
      "displayName": "Basharat hussain",
      "userId": "01707569703308499646"
     },
     "user_tz": -300
    },
    "id": "3JUxssUf5svT",
    "outputId": "891999e1-5a65-43b1-8907-312b7e3049ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the Model and Tokenizer\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# Qwen tokenizer needs a pad token set explicitly\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Important for batched inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2189,
     "status": "ok",
     "timestamp": 1756138064900,
     "user": {
      "displayName": "Basharat hussain",
      "userId": "01707569703308499646"
     },
     "user_tz": -300
    },
    "id": "YtWUE_iW8qEx",
    "outputId": "a0a77a89-15c0-4d33-88c8-42eccaab2d9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "# 2. Load and Preprocess the Dataset\n",
    "# We'll use a small subset of the databricks-dolly-15k dataset.\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# Select a small number of samples for a quick demonstration\n",
    "subset_size = 500\n",
    "dataset = dataset.select(range(subset_size))\n",
    "\n",
    "# Function to format the data into an instruction-following prompt\n",
    "def format_prompt(sample):\n",
    "    if sample[\"instruction\"] and sample[\"context\"]:\n",
    "        # Format for instruction with context\n",
    "        prompt = f\"### Instruction:\\n{sample['instruction']}\\n\\n### Context:\\n{sample['context']}\\n\\n### Response:\\n{sample['response']}{tokenizer.eos_token}\"\n",
    "    else:\n",
    "        # Format for instruction without context\n",
    "        prompt = f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['response']}{tokenizer.eos_token}\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Apply the formatting and tokenize the dataset\n",
    "dataset = dataset.map(format_prompt, remove_columns=list(dataset.features.keys()))\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # This is the corrected part\n",
    "    tokenized_output = tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "    # For causal language modeling, the labels are the same as the input_ids\n",
    "    tokenized_output[\"labels\"] = tokenized_output[\"input_ids\"].copy()\n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1613,
     "status": "ok",
     "timestamp": 1756138066510,
     "user": {
      "displayName": "Basharat hussain",
      "userId": "01707569703308499646"
     },
     "user_tz": -300
    },
    "id": "kAUBkC178wUR",
    "outputId": "9aa3bf07-5e07-4e33-a689-f500e786dbec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for LoRA fine-tuning...\n",
      "LoRA-enabled model trainable parameters:\n",
      "trainable params: 4,358,144 || all params: 1,548,072,448 || trainable%: 0.2815\n"
     ]
    }
   ],
   "source": [
    "# 3. Configure and Prepare LoRA\n",
    "# Prepare the model for k-bit training and apply LoRA\n",
    "print(\"Preparing model for LoRA fine-tuning...\")\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16, # Rank of the update matrices\n",
    "    lora_alpha=32, # A scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Layers to apply LoRA to\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"LoRA-enabled model trainable parameters:\")\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "executionInfo": {
     "elapsed": 231718,
     "status": "ok",
     "timestamp": 1756138298230,
     "user": {
      "displayName": "Basharat hussain",
      "userId": "01707569703308499646"
     },
     "user_tz": -300
    },
    "id": "TAFF-enZ80ay",
    "outputId": "dfc27ed2-99b5-48df-a207-2b843be8ff7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 03:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.085400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.889500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.580900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.511700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.816100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.584100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.525900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.8485932807922363, metrics={'train_runtime': 230.4224, 'train_samples_per_second': 2.17, 'train_steps_per_second': 0.542, 'total_flos': 2019377283072000.0, 'train_loss': 0.8485932807922363, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Train the Model\n",
    "output_dir = \"qwen1.5-1.5b-lora-dolly-finetuned\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True, # Use FP16 for faster training on GPU\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "print(\"Starting training...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGFtism19OzE"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1756138298579,
     "user": {
      "displayName": "Basharat hussain",
      "userId": "01707569703308499646"
     },
     "user_tz": -300
    },
    "id": "ETfTL4bb4t5x",
    "outputId": "0459a0aa-d196-413b-d7f6-4470ef2a31e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Saving model to qwen1.5-1.5b-lora-dolly-finetuned\n",
      "Fine-tuning successful! The LoRA weights are saved in the specified directory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Save the Fine-Tuned Model Locally\n",
    "# The trainer automatically saves the LoRA adapter weights\n",
    "print(f\"Training complete. Saving model to {output_dir}\")\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "print(\"Fine-tuning successful! The LoRA weights are saved in the specified directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYu3ZK6U9UVq"
   },
   "source": [
    "# Evaluate the locally saved and Finetuned Model (3 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqHS1ckm9TMC"
   },
   "outputs": [],
   "source": [
    "# 1. Load the base model\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the base model in 4-bit, just like during training\n",
    "model_id = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  # Change padding side for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1756138303558,
     "user": {
      "displayName": "Basharat hussain",
      "userId": "01707569703308499646"
     },
     "user_tz": -300
    },
    "id": "qUSLFiT54t27",
    "outputId": "4f3cd496-ff3b-48b4-ba06-596dd48ad5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapter from qwen1.5-1.5b-lora-dolly-finetuned...\n"
     ]
    }
   ],
   "source": [
    "# 2. Load local finetuned adapter\n",
    "\n",
    "# Path to your saved LoRA adapter weights\n",
    "adapter_path = \"qwen1.5-1.5b-lora-dolly-finetuned\"\n",
    "\n",
    "print(f\"Loading LoRA adapter from {adapter_path}...\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# You can optionally merge the adapter weights into the base model\n",
    "# This is useful for saving the final model for deployment and can improve inference speed.\n",
    "# model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10438,
     "status": "ok",
     "timestamp": 1756138313981,
     "user": {
      "displayName": "Basharat hussain",
      "userId": "01707569703308499646"
     },
     "user_tz": -300
    },
    "id": "w8uIbB-N4t0K",
    "outputId": "86b7cc0f-6a94-43cf-9259-49102a730328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n",
      "\n",
      "--- Generated Output ---\n",
      "### Instruction:\n",
      "What are the benefits of a plant-based diet?\n",
      "\n",
      "### Response: A plant-based diet offers many health benefits. The most well-known is that it can lower the risk of heart disease, strokes and cancer. Eating more fruits, vegetables, legumes and whole grains also helps to reduce blood pressure. And eating less meat may lower cholesterol levels. Plant-based diets have been associated with a longer lifespan as well. However, there's no clear evidence that they actually cause weight loss.\n"
     ]
    }
   ],
   "source": [
    "# 3. Test the prompt\n",
    "\n",
    "# Test the model with a sample prompt\n",
    "prompt = \"### Instruction:\\nWhat are the benefits of a plant-based diet?\\n\\n### Response:\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(\"Generating response...\")\n",
    "# Generate a response from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode and print the generated output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the full generated text\n",
    "print(\"\\n--- Generated Output ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADNTI2l34txC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
